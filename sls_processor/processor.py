# sls_processor/processor.py

import json
import logging
import time
import os
from typing import Dict, Any, Optional
from queue import Queue, Empty

# --- LangfuseDataProcessor å’Œ LangfuseSender ç±»çš„ä»£ç  ---
class LangfuseDataProcessor:
    """SLSæ—¥å¿—åˆ°Langfuseæ•°æ®çš„è½¬æ¢å¤„ç†å™¨ - é«˜æ€§èƒ½ç²¾ç®€ç‰ˆ"""
    
    @staticmethod
    def parse_ai_log(ai_log_str: str) -> Dict[str, Any]:
        """è§£æai_log JSONå­—ç¬¦ä¸²"""
        try:
            return json.loads(ai_log_str) if ai_log_str and ai_log_str.strip() and ai_log_str != '{}' else {}
        except json.JSONDecodeError:
            return {}
    
    @staticmethod
    def safe_int(value: Any) -> Optional[int]:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        if value is None or value == '' or value == '-':
            return None
        try:
            return int(float(value))
        except (ValueError, TypeError):
            return None
    
    @staticmethod
    def build_performance_metadata(ai_log: Dict, log_contents: Dict) -> Dict[str, Any]:
        """æ„å»ºæ€§èƒ½metadata - é¢„è¿‡æ»¤"""
        data = {}
        if (duration := LangfuseDataProcessor.safe_int(log_contents.get('duration'))):
            data["total_duration_ms"] = duration
        if (llm_duration := LangfuseDataProcessor.safe_int(ai_log.get('llm_service_duration'))):
            data["llm_service_duration_ms"] = llm_duration
        if (upstream := LangfuseDataProcessor.safe_int(log_contents.get('upstream_service_time'))):
            data["upstream_service_time_ms"] = upstream
        if (response_tx := LangfuseDataProcessor.safe_int(log_contents.get('response_tx_duration'))):
            data["response_tx_duration_ms"] = response_tx
        return data
    
    @staticmethod
    def build_request_metadata(log_contents: Dict) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚metadata - é¢„è¿‡æ»¤"""
        data = {}
        if (method := log_contents.get('method')):
            data["method"] = method
        if (path := log_contents.get('path')):
            data["path"] = path
        if (original_path := log_contents.get('original_path')):
            data["original_path"] = original_path
        if (response_code := LangfuseDataProcessor.safe_int(log_contents.get('response_code'))):
            data["response_code"] = response_code
        if (response_details := log_contents.get('response_code_details')):
            data["response_code_details"] = response_details
        if (user_agent := log_contents.get('user_agent')):
            data["user_agent"] = user_agent
        if (protocol := log_contents.get('protocol')):
            data["protocol"] = protocol
        if (authority := log_contents.get('authority')):
            data["authority"] = authority
        return data
    
    @staticmethod
    def build_infrastructure_metadata(log_contents: Dict) -> Dict[str, Any]:
        """æ„å»ºåŸºç¡€è®¾æ–½metadata - é¢„è¿‡æ»¤"""
        data = {}
        if (container_ip := log_contents.get('_container_ip_')):
            data["container_ip"] = container_ip
        if (namespace := log_contents.get('_namespace_')):
            data["namespace"] = namespace
        if (cluster_id := log_contents.get('cluster_id')):
            data["cluster_id"] = cluster_id
        if (route_name := log_contents.get('route_name')):
            data["route_name"] = route_name
        if (upstream_host := log_contents.get('upstream_host')):
            data["upstream_host"] = upstream_host
        return data
    
    @staticmethod
    def convert_to_langfuse_format(log_contents: Dict[str, Any]) -> Dict[str, Any]:
        """å°†SLSæ—¥å¿—è½¬æ¢ä¸ºLangfuseæ ¼å¼ - é«˜æ€§èƒ½ç‰ˆ"""
        ai_log = LangfuseDataProcessor.parse_ai_log(log_contents.get('ai_log', '{}'))
        
        # æ ¸å¿ƒå­—æ®µæå–
        trace_input = log_contents.get('question', '')
        trace_output = log_contents.get('answer', '') if log_contents.get('answer') != '-' else ''
        api_prefix = ai_log.get('api', '').split('@')[0] if ai_log.get('api') else ''
        response_code = LangfuseDataProcessor.safe_int(log_contents.get('response_code'))
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„æ„å»ºusage_detailsï¼ˆé¿å…é‡å¤æ£€æŸ¥ï¼‰
        usage_details = None
        if ai_log.get('input_token') or ai_log.get('output_token') or ai_log.get('total_token'):
            usage_details = {}
            if (input_tokens := LangfuseDataProcessor.safe_int(ai_log.get('input_token'))):
                usage_details['input'] = input_tokens
            if (output_tokens := LangfuseDataProcessor.safe_int(ai_log.get('output_token'))):
                usage_details['output'] = output_tokens
            if (total_tokens := LangfuseDataProcessor.safe_int(ai_log.get('total_token'))):
                usage_details['total'] = total_tokens
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„æ„å»ºtagsï¼ˆé¿å…filterå’Œlistæ“ä½œï¼‰
        tags = []
        if api_prefix:
            tags.append(api_prefix)
        if ai_log.get('response_type'):
            tags.append(ai_log.get('response_type'))
        if log_contents.get('_namespace_'):
            tags.append(log_contents.get('_namespace_'))
        if ai_log.get('model'):
            tags.append(f"model:{ai_log.get('model')}")
        
        # çŠ¶æ€æ ‡ç­¾
        if response_code:
            if 200 <= response_code < 300:
                tags.append("status:success")
            elif 400 <= response_code < 500:
                tags.append("status:client_error")
            elif response_code >= 500:
                tags.append("status:server_error")
            else:
                tags.append("status:other")
        else:
            tags.append("status:unknown")
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šç›´æ¥ç¡®å®šlevelï¼ˆé¿å…é‡å¤åˆ¤æ–­ï¼‰
        if response_code is None:
            level = "DEFAULT"
        elif 200 <= response_code < 300:
            level = "DEFAULT"
        elif 400 <= response_code < 500:
            level = "WARNING"
        elif response_code >= 500:
            level = "ERROR"
        else:
            level = "DEBUG"
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„æ„å»ºçŠ¶æ€æ¶ˆæ¯
        status_parts = []
        if response_code:
            status_parts.append(f"HTTP {response_code}")
        if log_contents.get('response_code_details'):
            status_parts.append(log_contents.get('response_code_details'))
        if ai_log.get('response_type'):
            status_parts.append(f"AI: {ai_log.get('response_type')}")
        status_message = " | ".join(status_parts) if status_parts else "Unknown"
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„æ„å»ºgenerationåç§°
        if ai_log.get('chat_round'):
            generation_name = f"{ai_log.get('api', 'AI Generation')} - Round {ai_log.get('chat_round')}"
        elif ai_log.get('model'):
            generation_name = f"{ai_log.get('api', 'AI Generation')} ({ai_log.get('model')})"
        else:
            generation_name = ai_log.get('api', 'AI Generation')
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šåˆ†åˆ«æ„å»ºmetadataå­ç»„ï¼ˆé¿å…åµŒå¥—clean_dictï¼‰
        metadata = {}
        
        # ç¯å¢ƒä¿¡æ¯
        if log_contents.get('_namespace_'):
            metadata["environment"] = log_contents.get('_namespace_', 'default')
        
        # æ€§èƒ½æŒ‡æ ‡
        if (performance := LangfuseDataProcessor.build_performance_metadata(ai_log, log_contents)):
            metadata["performance"] = performance
        
        # è¯·æ±‚ä¿¡æ¯
        if (request := LangfuseDataProcessor.build_request_metadata(log_contents)):
            metadata["request"] = request
        
        # åŸºç¡€è®¾æ–½ä¿¡æ¯
        if (infrastructure := LangfuseDataProcessor.build_infrastructure_metadata(log_contents)):
            metadata["infrastructure"] = infrastructure
        
        # å¯¹è¯ä¸Šä¸‹æ–‡
        chat_context = {}
        if ai_log.get('api'):
            chat_context["api_full_name"] = ai_log.get('api')
        if ai_log.get('chat_round'):
            chat_context["chat_round"] = ai_log.get('chat_round')
        if ai_log.get('response_type'):
            chat_context["response_type"] = ai_log.get('response_type')
        if ai_log.get('fallback_from'):
            chat_context["fallback_from"] = ai_log.get('fallback_from')
        if chat_context:
            metadata["chat_context"] = chat_context
        
        # ç½‘ç»œä¼ è¾“
        network = {}
        if (bytes_sent := LangfuseDataProcessor.safe_int(log_contents.get('bytes_sent'))):
            network["bytes_sent"] = bytes_sent
        if (bytes_received := LangfuseDataProcessor.safe_int(log_contents.get('bytes_received'))):
            network["bytes_received"] = bytes_received
        if log_contents.get('downstream_remote_address'):
            network["downstream_remote_address"] = log_contents.get('downstream_remote_address')
        if log_contents.get('upstream_local_address'):
            network["upstream_local_address"] = log_contents.get('upstream_local_address')
        if network:
            metadata["network"] = network
        
        # åŸå§‹è¿½è¸ªä¿¡æ¯
        original_trace = {}
        if log_contents.get('trace_id'):
            original_trace["sls_trace_id"] = log_contents.get('trace_id')
        if log_contents.get('request_id'):
            original_trace["request_id"] = log_contents.get('request_id')
        if log_contents.get('_time_'):
            original_trace["log_time"] = log_contents.get('_time_')
        if log_contents.get('start_time'):
            original_trace["start_time"] = log_contents.get('start_time')
        if original_trace:
            metadata["original_trace"] = original_trace
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šç›´æ¥æ„å»ºç»“æœï¼ˆé¿å…clean_dataé€’å½’è°ƒç”¨ï¼‰
        result = {
            "trace_name": ai_log.get('api', 'AI Request'),
            "trace_input": trace_input,
            "trace_output": trace_output,
            "generation_name": generation_name,
            "generation_input": trace_input,
            "generation_output": trace_output,
            "level": level,
            "status_message": status_message,
            "tags": tags
        }
        
        # æ·»åŠ å¯é€‰å­—æ®µ
        if ai_log.get('consumer') or log_contents.get('consumer'):
            result["user_id"] = ai_log.get('consumer', log_contents.get('consumer', ''))
        if ai_log.get('chat_id'):
            result["session_id"] = ai_log.get('chat_id')
        if ai_log.get('model'):
            result["model"] = ai_log.get('model')
        if usage_details:
            result["usage_details"] = usage_details
        if log_contents.get('start_time'):
            result["start_time"] = log_contents.get('start_time')
        if metadata:
            result["metadata"] = metadata
        
        return result

class LangfuseSender:
    """Langfuseæ•°æ®å‘é€å™¨"""
    
    def __init__(self):
        try:
            # --- âœ¨ æ ¸å¿ƒä¿®æ­£ï¼šç›´æ¥åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œè®©å®ƒè‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ ---
            from langfuse import get_client
            self.langfuse = get_client()
            
            # ä» os.environ è¯»å– HOST ç”¨äºæ‰“å°ï¼Œç¡®ä¿ä¿¡æ¯æ¥æºä¸€è‡´
            host = os.environ.get('LANGFUSE_HOST', 'N/A')
            logger.info(f"âœ… Langfuseå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå°†è¿æ¥åˆ°: {host}")

        except Exception as e:
            logger.error(f"âŒ Langfuseå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise


    def send_trace_with_generation(self, data: Dict[str, Any]) -> bool:
        """å‘é€traceå’Œgeneration"""
        try:
            with self.langfuse.start_as_current_span(
                name=data.get('trace_name', 'AI Request'),
                input=data.get('trace_input')
            ) as root_span:
                
                root_span.update_trace(
                    input=data.get('trace_input'),
                    output=data.get('trace_output'),
                    user_id=data.get('user_id'),
                    session_id=data.get('session_id'),
                    tags=data.get('tags', []),
                    metadata=data.get('metadata', {})
                )
                
                root_span.update(output=data.get('trace_output'), metadata=data.get('metadata', {}))
                
                with self.langfuse.start_as_current_observation(
                    as_type='generation',
                    name=data.get('generation_name', 'AI Generation'),
                    input=data.get('generation_input'),
                    model=data.get('model')
                ) as generation:
                    
                    update_params = {
                        'output': data.get('generation_output'),
                        'level': data.get('level', 'DEFAULT'),
                        'status_message': data.get('status_message', ''),
                        'metadata': data.get('metadata', {})
                    }
                    
                    if data.get('usage_details'):
                        update_params['usage_details'] = data.get('usage_details')
                    
                    generation.update(**update_params)
            
            return True
        except Exception as e:
            logger.error(f"å‘é€åˆ°Langfuseå¤±è´¥: {e}")
            return False
    
    def flush(self) -> bool:
        try:
            self.langfuse.flush()
            return True
        except Exception as e:
            logger.error(f"Langfuseåˆ·æ–°å¤±è´¥: {e}")
            return False


# --- æ–°å¢çš„å¤„ç†å™¨ä¸»å¾ªç¯ ---
logger = logging.getLogger(__name__)

def write_to_dead_letter_queue(log_data: Dict[str, Any]):
    """å°†å¤„ç†å¤±è´¥çš„æ—¥å¿—å†™å…¥æœ¬åœ°æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­æ’æŸ¥ã€‚"""
    try:
        with open(DEAD_LETTER_QUEUE_FILE, "a") as f:
            f.write(json.dumps(log_data) + "\n")
        logger.warning(f"æ—¥å¿—å·²å†™å…¥æ­»ä¿¡é˜Ÿåˆ—: {DEAD_LETTER_QUEUE_FILE}")
    except Exception as e:
        logger.error(f"å†™å…¥æ­»ä¿¡é˜Ÿåˆ—å¤±è´¥: {e}")

def process_logs_from_queue(log_queue: Queue, stop_event):
    """
    ä»é˜Ÿåˆ—ä¸­è·å–æ—¥å¿—ï¼Œå¤„ç†å¹¶å‘é€åˆ°Langfuseï¼Œå¢åŠ äº†é‡è¯•å’Œæ­»ä¿¡é˜Ÿåˆ—é€»è¾‘ã€‚
    """
    sender = LangfuseSender()
    stats = {'processed': 0, 'success': 0, 'error': 0, 'skipped': 0, 'retries': 0, 'dead_letter': 0}
    start_time = time.time()
    
    # --- æ–°å¢ï¼šé‡è¯•å‚æ•°é…ç½® ---
    MAX_RETRIES = 3
    RETRY_DELAY_SECONDS = 5

    logger.info("ğŸš€ Langfuseå¤„ç†å™¨å·²å¯åŠ¨ (å¢å¼ºç‰ˆï¼šå¸¦é‡è¯•å’Œæ­»ä¿¡é˜Ÿåˆ—)...")

    while not stop_event.is_set():
        try:
            log_data = log_queue.get(timeout=1.0)
        except Empty:
            continue # é˜Ÿåˆ—ä¸ºç©ºï¼Œæ­£å¸¸ï¼Œç»§ç»­å¾ªç¯
        
        stats['processed'] += 1
        trace_id = log_data.get('trace_id', 'N/A')

        langfuse_data = LangfuseDataProcessor.convert_to_langfuse_format(log_data)
        
        # --- æ–°å¢ï¼šå¸¦é‡è¯•çš„å‘é€é€»è¾‘ ---
        sent_successfully = False
        for attempt in range(MAX_RETRIES):
            if sender.send_trace_with_generation(langfuse_data):
                stats['success'] += 1
                sent_successfully = True
                logger.info(f"âœ… å‘é€æˆåŠŸ: Trace [ {trace_id[:16]}... ] | API [ {langfuse_data.get('trace_name', 'N/A')} ]")
                break # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            else:
                stats['retries'] += 1
                logger.warning(f"âŒ å‘é€å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): Trace [ {trace_id[:16]}... ]")
                if attempt < MAX_RETRIES - 1:
                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
                    time.sleep(RETRY_DELAY_SECONDS)
        
        # --- æ–°å¢ï¼šå¤„ç†æœ€ç»ˆå¤±è´¥çš„æƒ…å†µ ---
        if not sent_successfully:
            stats['error'] += 1
            stats['dead_letter'] += 1
            logger.error(f"ğŸš¨ å‘é€æœ€ç»ˆå¤±è´¥ï¼Œå·²æ”¾å¼ƒ: Trace [ {trace_id[:16]}... ]")
            write_to_dead_letter_queue(log_data)

        # --- ä¿®æ”¹å®šæœŸç»Ÿè®¡æ—¥å¿—çš„é¢‘ç‡å’Œå†…å®¹ ---
        if stats['processed'] % 50 == 0:
            # ... (ç»Ÿè®¡æ—¥å¿—æ‰“å°éƒ¨åˆ†ä¿æŒä¸å˜) ...
            pass

    logger.info("â„¹ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åˆ·æ–°Langfuse SDK...")
    sender.flush()
    logger.info("âœ… Langfuseå¤„ç†å™¨å·²æˆåŠŸå…³é—­ã€‚")